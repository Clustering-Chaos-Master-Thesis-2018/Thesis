\chapter{Evaluation}
\label{chap:evaluation}
\begin{newtext}{New text}
In this chapter, we evaluate our clustering protocol. We begin by defining the topologies used during the evaluation. Next, we describe the metrics used to measure and compare the success of tests. We continue with an evaluation for different values of our clustering parameters, which are presented in \cref{design:configuration-parameters}, where we determine what configurations of the parameters work best for different topologies. Moreover, we present a comparison between our clustering implementation and the original \atwo{} Synchrotron. We end this chapter with a discussion.
\end{newtext}

\section{Test Setup}
To test our clustering implementation, we run an unmodified version of the \atwo{} system and compare it to our clustering implementation. In this section, we describe our test setup, which topologies we test on, and which metrics we use to measure our results. 


\subsection{Test Environments}
We evaluate our clustering implementation in two different ways, on the Cooja simulator \cite{Osterlind2006-cooja-introduction} and the Flocklab testbed \cite{Lim2013-flocklab-introduction}. Cooja is the default simulator for ContikiOS \cite{Dunkels2004-contiki-introduction}, the operating system that Chaos and \atwo{} are implemented on \cite{chaos-introduction-paper, a2-introduction-paper}.

\subsection{Topology Details}
\label{subsec:evaluation-topology-details}
In Cooja, we create network topologies with 50 and 200 nodes placed randomly in different areas, creating different node densities. We start with 50 nodes in a 100x100 $m^2$ area and increase the side of the square in steps of 300 meters all the way to 2500x2500 $m^2$. All network topologies are fixed, they do not change between tests. In all tests, we simulate each network for 30 minutes, which gives us 600 rounds of data. We run most of our tests using 50 nodes for two reasons. First, 50 nodes are enough to see that the Clustering service creates several clusters. Second, running these simulations takes a significant amount of time, and it does not scale well when increasing the number of nodes. Currently, simulating 30 minutes with 50 and 200 nodes takes between 2-3 hours and 30-50 hours respectively. However, when comparing our clustering process to the \atwo{} system we also simulate 200 nodes to see the effects of scaling up the network. Moreover, we chose the different areas of deployment from the fact that in Cooja, a network with area 100x100 meters ensures a 1-hop network and at 2500x2500 meters the network is sparse enough for clustering to be disadvantageous.

The Flocklab testbed consists of 27 nodes deployed both indoors and outdoors at ETH Zurich University. Flocklab provides several services such as \textit{general purpose input output} (GPIO) pin tracing, power profiling, and adjustable supply voltage \cite{Lim2013-flocklab-introduction}. These services makes it easy to measure and evaluate the code that is running on the nodes. Our tests log data to the serial output port of an observer node, Flocklab aggregates the data and sends it to us at the end of each test.

\subsection{Metrics}
\begin{newtext}{newtext}
We consider four metrics when evaluating clustering on \atwo{}: reliability, stability, latency and energy usage. These metrics are described in detail below, first with an explanation for understandability, then mathematically for exactness. The metrics are used since they provide a good evaluation basis from which to argue if we fulfil the goals of this thesis.

% Something about why we use these metrics.
% Because they evaluate our problem statement properly?

% most basic explanation for the smallest contribution
% more in depth explanation if something is unclear, (especially for reliability)
% how is the metric presented in a graph

\paragraph*{Reliability} is a measurement of how well an application runs on the network; thus, it is counted during Max application rounds. It is the number of successfully executed rounds of the Max application counted over all nodes, divided by the total number of executed rounds of the Max application over all nodes. Successful execution of the Max application for a node means the node learns the correct maximal value. In a cluster round, the correct value for a node is the maximal value proposed in its cluster. In a CH round, the correct value is the maximal value learnt by all CHs in the previous cluster round and proposed in this round.

We can define this calculation mathematically as follows. We define $n$ as the number of nodes in the network, $m_i$ is the number of nodes which executed the Max application in round $i$, and $M_i$ is the number of nodes which executed the Max application successfully in round $i$, $M_i \leq m_i \leq n$. In a network which executes $r$ number of rounds, we define reliability as 

\begin{equation}
    \frac{\sum\limits_{i=1}^{r}{M_i}}{\sum\limits_{i=1}^{r}{m_i}}.
\end{equation}

\newpage
\paragraph*{Stability} is a measurement of how often topology changes occur. During normal execution, in a stable network, we expect only the Max application to run. Thus, we measure the ratio of nodes which do not schedule the Max application. We measure this per round and then calculate the mean over all rounds.

In detail, we expect the Max application to run in the rounds $R = [36,199] \cup [236,399] \cup [436, 599]$. The other rounds are used for the Clustering service. Stability is then defined as 

\begin{equation}
    \frac{\sum\limits_{i \in R}{m_i/n}}{|R|}.
\end{equation}


\paragraph*{Latency} is a measurement of how long it takes for an application to terminate. For one node in one round it is the number of slots from the start of a round until the slot in which the node powers down.  Specifically, if the number of slots required for node $j$ in round $i$ to terminate is $o_{ij}$, then latency is defined as
\begin{equation}
\frac{\sum\limits_{i=1}^{r}{ \sum\limits_{j=1}^{n}{o_{ij} /m_i} }}{r}.
\end{equation}




\paragraph*{Energy usage}
We measure the energy consumption of every node using the built-in energy estimation module Energest in Contiki. The energy measurement includes the total energy consumed by the CPU and radio. We then calculate the average energy consumed over all nodes per Energest time unit, which is approximately 1/128 of a second. Only estimating energy usage in software is a limitation to our testing, but it is adequate for making comparisons.

The formula for energy usage is a per-node average, if $e_{ij}$ is the energy used by node $i$ in round $j$ then our measurement for energy usage is

\begin{equation}
    \frac{1}{r*n} \sum\limits_{i=1}^{r}{ \sum\limits_{j=1}^{n}{e_{ij}} }.
\end{equation}



% Old reliability text
%We measure reliability as the number of rounds all nodes receive the correct final value divided by the total number of rounds the max protocol is supposed to be running; that is, we remove the rounds which the nodes spend running the clustering, demote, and Join services. During cluster rounds, we count the round as a success if all nodes in every cluster have learned their local maximum. During CH rounds we count the round as a success if all CHs have learned the maximum global value.

These metrics are inspired from the work of Landsiedel et al.~and Al Nahas et al.~ on the original Chaos protocol and \atwo{} system \cite{chaos-introduction-paper, a2-introduction-paper}. However, we have modified the reliability metric and introduced stability as a new metric. Previous work defines reliability as the percentage of rounds in which all nodes reach completion, no matter the service or application. We, on the other hand, count reliability in one round as the ratio of nodes which finds the correct maximal value. Reliability is thus only counted in rounds in which the Max application is executed. Furthermore, a high stability means that there are more data points to use when calculating the reliability of a test, since higher stability means that the max application executed more often.
\end{newtext}

\subsection{Limitations}
We limit our evaluation in the following ways. We only compare our clustering implementation to \atwo{}. Furthermore, we limit the testing to Cooja simulations and tests run on the Flocklab testbed. We believe that the Cooja simulations provide valuable results since we gain freedom in choosing topologies. In contrast, the Flocklab testbed is a WSN with real nodes, but the number of nodes in the network limits us; the Flocklab testbed contains 27 nodes. While also being on the sparser side compared to the topologies we test in Cooja, the Flocklab testbed give us very restricted results regarding the effects of running a clustered network versus the original \atwo{} system.
%this makes it a network which is hard to cluster.

Furthermore, we only simulate networks with 50 nodes when testing the parameters. When comparing clustering to the original \atwo{} system we also simulate networks with 200 nodes. We choose these node counts since we are able to simulate 50 nodes within a reasonable amount of time, allowing us to repeat those tests many times. The 200 node networks, on the other hand, takes a very long time to simulate, which is why we only simulated these large networks when comparing our work to the \atwo{} system.

Lastly, we only evaluate clustering using the max application. The max application is the most straightforward application currently implemented in the \atwo{} system. We have not considered any other application, but it should be possible to modify most of the applications to fit our clustering protocol.

\iffalse
% Vet inte om eller var vi ska ta med att vi inte mäter throughput, eller varför vi inte mäter det. Det skulle kunna vara future work. Men det känns onödigt då det finns mycket annat att göra innan en så pass rigorös utvärdering är lämplig.
Lastly, the parameters we evaluate optimal values for are restricted to competition radius, minimum node count, nodes per cluster ratio and round re-synchronisation threshold. 
% no throughput or evaluation based on payload size
\fi

\section{Clustering Parameters}
\label{sec:clustering-parameters}
In this section, we experimentally search for optimal values for the parameters of our clustering algorithm. The parameters are \emph{round re-synchronisation threshold}, \emph{competition radius}, \emph{minimum cluster size}, and \emph{nodes per cluster ratio}; the purpose of these parameters is described in \cref{design:configuration-parameters}. We run each test three to six times and plot the reliability of each test and for each network topology.

We are primarily interested in reliability for these tests since reliability displays the most significant changes. There are no significant differences in the latency for different values of these parameters; we include the latency results in \cref{app:a} for completeness but will not comment on them any further for the parameter tests.

We use a different number of topologies depending on which parameter we evaluate. When evaluating the parameter competition radius, we test on all network topologies we describe in \cref{subsec:evaluation-topology-details} since this parameter is very dependent on the density of the network.

However, when we test the parameters minimum node count and nodes per cluster ratio, we use a slightly different setup. For both of those parameters, we only run tests on topologies ranging from 100x100 to 1300x1300 meters in size, using competition radius 1. We argue that increasing the competition radius while simultaneously increasing the area of the networks will not give any new significant results. By excluding the network topologies with a larger area, we could repeat each configuration for these tests a total of six times, instead of three.

Also, to limit the scope of our evaluation, we use the values listed in \cref{tab:parameter-default-values} for the parameters that we are not currently evaluating. Initial tests using these parameter values gave satisfying results. However, to get a better understanding, we could evaluate how the different parameters interact with each other, but we will not perform those evaluations in this thesis.

\begin{table}[bt]
\centering
\caption{The parameters we evaluate and their default values}
\label{tab:parameter-default-values}
\begin{tabular}{r|l}
\textbf{Parameter}                 & \textbf{Value} \\ \hline
Competition Radius                 & 1              \\
Minimum node count                 & 4              \\
Nodes per cluster ratio            & 10             \\
Round re-synchronisation threshold & 3             
\end{tabular}
\end{table}

\subsection{Round Re-synchronisation Threshold}
The \emph{round re-sync threshold} parameter is not directly related to the clustering algorithm. Instead, it is a setting that controls a fundamental part of \atwo{}. Round re-sync threshold is a numeric parameter that determines how many rounds a node can execute without receiving any packet before trying to associate with the network again. 

We briefly evaluate different values for round re-sync threshold since initial testing showed that our clustering implementation is unstable. We started our tests using a re-sync threshold of 1 but that proved to be too strict, and we did not get satisfying results. We evaluate three different values of the round re-sync threshold (1, 2, and 3), for each value of re-sync threshold we evaluate three different values of competition radius (1, 2, and 3), which gives us a total of 9 test configurations. We chose to use different values of competition radius when evaluating this parameter in order to see the effects of different round re-sync threshold on larger networks.

Looking at the results in \cref{fig:resync-treshold-tests} and \cref{fig:stability-resync-treshold-tests}, we see an increase in both reliability and stability when we increase the re-sync threshold.


The reason for this dramatic increase is due to two things. First, our clustering implementations instability is due to specific scenarios occurring, such as when switching between initiators, which we describe in \cref{subsec:implementation_the-initiator}. Second, when clustering a network, each node generally has fewer chances to successfully receive a packet from another node since it will only listen to nodes from its cluster. This is apparent in our testing setup since we use a relatively small number of nodes and we run many tests on sparser networks, specifically 1300x1300 meters and upwards. 

From this evaluation, we conclude that using a re-sync threshold of 3 is currently the best value for our clustering implementation. The plots in \cref{fig:resync-treshold-tests} suggest a trend of higher reliability with an increasing re-sync threshold. However, we did not test larger values of this parameter due to time constraints. All further evaluation of our clustering implementation uses a re-synchronisation threshold of 3.

\begin{figure}[bt]
\centering
\begin{subfigure}{\textwidth}
    \centering
    \includegraphics[width=\textwidth, keepaspectratio]{figure/Results/ParameterEvaluation/ResyncThreshold1_Reliability.pdf}
    \caption{Re-synchronisation threshold 1.}
    \label{subfig:resync-treshold-1}
\end{subfigure}
\begin{subfigure}{\textwidth}
    \centering
    \includegraphics[width=\textwidth, keepaspectratio]{figure/Results/ParameterEvaluation/ResyncThreshold2_Reliability.pdf}
    \caption{Re-synchronisation threshold 2.}
    \label{subfig:resync-treshold-2}
\end{subfigure}
\begin{subfigure}{\textwidth}
    \centering
    \includegraphics[width=\textwidth, keepaspectratio]{figure/Results/ParameterEvaluation/ResyncThreshold3_Reliability.pdf}
    \caption{Re-synchronisation threshold 3.}
    \label{subfig:resync-treshold-3}
\end{subfigure}

    \caption{Competition radii tests for different values of resynchronisation threshold. The reliability increases as the re-sync threshold increases.}
    \label{fig:resync-treshold-tests}
\end{figure}

\begin{figure}[bt]
\centering
\begin{subfigure}{\textwidth}
    \centering
    \includegraphics[width=\textwidth, keepaspectratio]{figure/Results/ParameterEvaluation/ResyncThreshold1_Stability.pdf}
    \caption{Re-synchronisation threshold 1.}
    \label{subfig:stability-resync-treshold-1}
\end{subfigure}
\begin{subfigure}{\textwidth}
    \centering
    \includegraphics[width=\textwidth, keepaspectratio]{figure/Results/ParameterEvaluation/ResyncThreshold2_Stability.pdf}
    \caption{Re-synchronisation threshold 2.}
    \label{subfig:stability-resync-treshold-2}
\end{subfigure}
\begin{subfigure}{\textwidth}
    \centering
    \includegraphics[width=\textwidth, keepaspectratio]{figure/Results/ParameterEvaluation/ResyncThreshold3_Stability.pdf}
    \caption{Re-synchronisation threshold 3.}
    \label{subfig:stability-resync-treshold-3}
\end{subfigure}

    \caption{Competition radii tests for different values of resynchronisation threshold. The reliability increases as the re-sync threshold increases.}
    \label{fig:stability-resync-treshold-tests}
\end{figure}

\begin{figure}[bt]
    \centering
    \includegraphics[width=\textwidth, keepaspectratio]{figure/Results/ParameterEvaluation/CompetitionRadius_Reliability.pdf}
    \caption{The reliability for different network sizes. Each network size has been tested with competition radius 1, 2, and 3 (from left to right).}
    \label{fig:comp-radius-reliability}
\end{figure}

\subsection{Competition Radius}
Competition radius is a distance metric measured in hops. A node chooses a CH from all CHs that are closer than the value of the competition radius. We evaluate competition radius for the values 1,2 and 3, which ensures that we test the minimum value of 1 but also keep somewhat local clusters. When looking at the different competition radii, we are mainly interested in how the reliability varies when the density of the network decreases. We show the average reliability and the standard deviation for each network setup and each competition radius in \cref{fig:comp-radius-reliability}. We see some expected results, for networks that are sparse (e.g. network sizes from 1300x1300 to 2500x2500 meters) using a competition radius of 1 results in worse reliability than for competition radius 2 or 3. We probably get lower reliability for lower density networks because our clustering algorithm will create too many clusters and make the network even sparser, hindering communication. Even though reliability increases when we use competition radius 2 or 3 in large networks it is not by too much, it is still inefficient to cluster large networks with few nodes. 

Furthermore, we do not see any significant difference in the reliability for dense networks when increasing the competition radius. The reliability does not change because in very dense networks with a one-hop diameter, such as the network with an area of 100x100 meters, the competition radius is effectively always one since there are no nodes more than one hop away from any other node. The difference in reliability for the 100x100 area network seen in \cref{fig:comp-radius-reliability} is noise due to instabilities in our clustering implementation.

\subsection{Minimum Node Count}
CHs use the parameter minimum node count during the Demote service to decide if they should demote themselves because their clusters are too small. We evaluate this parameter using three different values 2, 4 and off; the off value indicates that a CH will never demote itself due to having too few nodes in its cluster. We show the mean reliability and standard deviation of these tests in \cref{fig:min-node-count-reliability}.

Looking at the reliability of the different network topologies we see no clear correlation between minimum node count and the reliability on the same topology. The reliability drops, as expected, the sparser the network is.

There are several potential reasons why we can not see any significant difference between different values for this parameter, which can not be determined by looking at the reliability graph alone. First, the impact of having a mix of small and large clusters in a network could be negligible, compared to having more equally sized clusters. The second reason could be that the clustering algorithm did not create clusters that were considered to be too small, then the parameter would not have any effect on the reliability. To further investigate the effects of this parameter we could increase the value of the parameter. However, with only 50 nodes in the network, increasing it further than we have already done could result in the clustering algorithm creating too few clusters. However, we have not performed evaluation based on the number of clusters created in a network and can therefore not provide any results on this.



\begin{figure}[bt]
    \centering
    \includegraphics[width=\textwidth, keepaspectratio]{figure/Results/ParameterEvaluation/MinNodeCount_Reliability.pdf}
    \caption{The reliability for different network sizes. Each network has been tested with minimum node count values off, 2, and 4 (from left to right).}
    \label{fig:min-node-count-reliability}
\end{figure}

\subsection{Nodes per Cluster Ratio}
Nodes use the \emph{nodes per cluster ratio} parameter to determine if they may elect themselves even if they have heard another CH within the chosen competition radius. The parameter describes a ratio of how many neighbours needs to be in the vicinity of the node relative to how many CHs it has as neighbours. We test the values 5, 10, 15, and off for this parameter. The value off means a CH may never announce itself if it has heard another CH within the network's competition radius; it is comparable to having an arbitrarily large value for the parameter. The results can be seen in \cref{fig:nodes-per-cluster-ratio-reliability}.

Looking at the results, we can see that fewer clusters yields a higher reliability. When max node count is turned off, dense networks (100x100) only elect one CH which mostly makes the network run like \atwo{} without clustering, hence the excellent reliability results. Similarly, only one CH is elected in four out of the six tests comprising the data labelled as 400x400 with max node count set to off. The two cases in which more clusters were created are the tests with the lowest reliability, causing a lower mean and a more substantial standard deviation compared to the 100x100 tests. We can also see that when we use a value of 5, the reliability drops significantly since too many clusters are created; for these tests, approximately 20 clusters were created. Consequently, as the value of \emph{nodes per cluster ratio} increases, we also see an increase in the mean reliability.



\begin{figure}[bt]
    \centering
    \includegraphics[width=\textwidth, keepaspectratio]{figure/Results/ParameterEvaluation/MaxNodeCount_Reliability.pdf}
    \caption{The reliability for different network sizes. Each network size has been tested with max node count off, 5, 10, and 15 (from left to right).}
    \label{fig:nodes-per-cluster-ratio-reliability}
\end{figure}


\subsection{Discussion}
The results we see in the parameter evaluation vary greatly, both when looking at a single test but also when looking at the different evaluations of the parameters combined. For some parameters, like competition radius, we see a significant increase in reliability as the network area increases for higher values of competition radius. For the parameter minimum node count, on the other hand, we see almost no difference in the reliability when the value of the parameter changes. From these results, we can give some general remarks about each of the parameters.


For competition radius, we see that this parameter is useful in creating reliable clusters since this is the parameter that overall had the most significant impact on the reliability of a test when increasing the network area. 

For minimum node count, the results we got from these tests suggest that this parameter does not have an impact on the overall reliability of the tests we ran. However, even if this is true, there is still a bad scenario that can occur which this parameter will solve. The scenario is that a node announces itself as CH, but no other nodes join its cluster. When a CH is alone in its cluster, it will, in the current implementation, only run the join application and never the max application. A CH not running the max application will result in lower reliability since we expect all CHs to receive either the local or global maximum value at the end of each round.

We can see an example of this bad scenario occurring by looking at an individual test from the evaluation of minimum node count. This test, which ran on the 700x700 topology, suffered from a bug causing a CH to form a cluster by itself resulting in the reliability $26\%$. Other tests on the same topology achieved much higher reliability, with the highest being $99.4\%$. There are many possible causes to why there might be such a significant difference between the reliability in the best and worst tests. However, we believe the most critical factor is that the test with the lower reliability had a cluster with a single node in it, for 170 of the rounds it should have been running the max application. We show the clusters created for that particular run in \cref{fig:min-node-count-example}, as we can see the CH with ID 12 at the top does not have any other node in its cluster.

\begin{figure}[bt]
    \centering
    \includegraphics[width=0.5\textwidth]{figure/Results/Discussion/MinNodeCountExample.pdf}
    \caption{An example of a clustering in a 700x700 network which resulted in one CH (node 12) being alone in its cluster. Each colour represents a different cluster, and the larger nodes are CH.}
    \label{fig:min-node-count-example}
\end{figure}

Last, the nodes per cluster ratio parameter worked as we expected. The idea behind this parameter is that since clustering a network should be desirable, we want to make our Clustering service less aggressive when it determines which nodes are allowed to become cluster heads. However, because of the instability in our clustering, the results we got was effectively the opposite, when fewer clusters were created, we got better reliability. Suggesting that this parameter should always be completely turned off.

Nonetheless, we can still consider the parameter useful since it addresses a shortcoming in the algorithm design. For our purpose, the most relevant metric for this parameter is not the reliability of the tests but rather the number of clusters created. We have not performed a rigorous analysis of the results in this regard; however, the data we have looked at suggests that the parameter performs its intended function.



\section{Comparing \atwo{} with Clustering}
In this section, we outline the results we get from evaluating \atwo{} with clustering and compare those results with the results we get from our evaluation of the original \atwo{} system. We show results for reliability, latency, and estimated energy usage for all tests we run. We begin by describing the reliability results since they are the most varying and the relevance of the results about latency and energy depend on the reliability of a test.

Note that we have excluded data from two tests (50 nodes in the 2500x2500 topology and 200 nodes in the 400x400 topology) since they suffered the scenario we describe in \cref{subsec:implementation_the-initiator}, i.e., the original initiator starts associating early causing the whole network to get stuck associating. This happened in one out of the three tests we run for each of the two configurations, which is why only two points are visible for those topologies. We exclude these tests since they almost never ran the max application, which means that we have no data about the reliability or latency. Furthermore, when the nodes associate they use more energy giving us two outliers in the test results, which do not say anything useful about how much energy our clustering protocol consumes.

Furthermore, from our evaluation of the clustering parameters in \cref{sec:clustering-parameters}, we know that different values of some parameters drastically affect the performance of our clustering protocol, especially competition radius. Consequently, we have used different values for some parameters for different network topologies. We change the value of the parameter competition radius as the network size increases for the topologies consisting of 50 nodes, to not create too many clusters in already sparse networks. Additionally, we change the value of the parameter nodes per cluster ratio for the two smallest topologies in the 200 nodes test (100x100 and 400x400), to force the algorithm to create an appropriate amount of clusters in dense networks. We list all topologies we test on and the parameter values that change for the tests in \cref{app:parameter-values-for-the-atwo-comparison}.


\begin{figure}[bt]
    \centering
    \begin{subfigure}{0.7\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/Results/ChaosComparison/ChaosComparison_50_Reliability.pdf}
        \caption{Networks with 50 nodes.}
        \label{subfig:reliability-50-nodes}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.7\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/Results/ChaosComparison/ChaosComparison_200_Reliability.pdf}
        \caption{Networks with 200 nodes.}
        \label{subfig:reliabilty-200-nodes}
    \end{subfigure}
    \caption{Reliability comparison between \atwo{} with clustering and original \atwo{}.}
    \label{fig:reliability-result}
\end{figure}
\subsection{Reliability}
\label{subsec:evaluation-reliability}
As seen in \cref{fig:reliability-result}, \atwo{} outperforms the clustering implementation in reliability in all cases. However, we see some promising results for the clustering implementation in the topology with 50 nodes in an area of 1000x1000 meters, \cref{subfig:reliability-50-nodes}, where two out of three tests has a reliability of more than $97.5$ percent. Both of these tests produced somewhere between two to four clusters in the network. At 1300x1300 meters in \cref{subfig:reliability-50-nodes}, the reliability drops to just below $50\%$ and then continues to decrease as the area of the networks increase. Looking at the results for 50 versus 200 nodes in \cref{subfig:reliability-50-nodes} and \cref{subfig:reliabilty-200-nodes} we compare network topologies from 100x100 up to 1300x1300 meters. When increasing the node count from 50 to 200, the reliability of all tests drop, except for the 1300x1300 topology in the clustering case.


The drop in reliability for \atwo{} is probably due to interference; Al Nahas et al.~\cite{a2-introduction-paper} have already observed and resolved this issue in dense networks by using parallel channels, which we describe in \cref{subsec-frequency-agility}. As we have disabled parallel channels, the reliability is affected. Similarly to parallel channels, clustering should also artificially lower network density and resolve the problem of interference. Hence, we expected a more significant drop in reliability from \atwo{} than in the clustering solution. However, this is not what we observe in the comparison between 50 and 200 nodes. Instead, we see that reliability does not scale to dense networks well with clustering. 

The reliability increases in the 1300x1300 network when moving from 50 to 200 nodes. One cause could be the node density on the 1300x1300 network with 200 nodes is comparable to that of the 700x700 network with 50 nodes. As can be seen in \cref{subfig:reliability-50-nodes}, we achieve good results on the 700x700 network with 50 nodes motivating that the node density is suitable for clustering. Further evaluation would include runs of 200 node networks for network sizes bigger than 1300x1300 meters to see if the trend has been shifted. However, it could also be the case that the tests were lucky with the clusters created.

\begin{figure}[bt]
    \centering
    \begin{subfigure}{0.7\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/Results/ChaosComparison/ChaosComparison_50_Latency.pdf}
        \caption{Networks with 50 nodes.}
        \label{subfig:latency-50-nodes}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.7\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/Results/ChaosComparison/ChaosComparison_200_Latency.pdf}
        \caption{Networks with 200 nodes.}
        \label{subfig:latency-200-nodes}
    \end{subfigure}
    \caption{Latency comparison between \atwo{} with clustering and original \atwo{}.}
    \label{fig:latency-results}
\end{figure}

\subsection{Latency}
We show the mean latency and standard deviation for each test we run in \cref{fig:latency-results}. Looking at the tests using 50 nodes (\cref{subfig:latency-50-nodes}) our latency is on average a little lower than \atwo{} but with a higher standard deviation. We see a similar trend for 200 nodes (\cref{subfig:latency-200-nodes}), the standard deviation is still high, however, our clustering consistently has lower average latency for all topologies tested. 

We expect to see a lower latency on average, especially for 200 nodes, since each cluster has to handle a smaller amount of nodes while \atwo{} without clustering always has to handle all 200 nodes in the network. However, we have to take into account that our solution has much lower reliability than \atwo{}. Since we only measure the latency when the max application is running, if some nodes spend much time either associating or running another application (such as the Join service) we do not count the latency for those nodes. Thus, for tests with lower reliability, we will have fewer data points for the latency measurements than for tests with higher reliability.

Furthermore, we have to take into account that we split the application rounds into cluster and cluster head rounds. To spread the same information to the CH nodes as \atwo{} does in one round, we need to run first a cluster round and then a CH round, which means that we effectively require two rounds to complete an application, rather than one. Thus we could argue that the actual latency for our application is twice the number that we depict in the graph, which would make our solution considerably slower than \atwo{}.

\begin{figure}[bt]
    \centering
    \begin{subfigure}{0.7\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/Results/ChaosComparison/ChaosComparison_50_Energy.pdf}
        \caption{Networks with 50 nodes.}
        \label{subfig:energy-50-nodes}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.7\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figure/Results/ChaosComparison/ChaosComparison_200_Energy.pdf}
        \caption{Networks with 200 nodes.}
        \label{subfig:energy-200-nodes}
    \end{subfigure}
    \caption{Energy comparison between \atwo{} with clustering and original \atwo{}.}
    \label{fig:energy-results}
\end{figure}


\subsection{Energy}
In \cref{fig:energy-results}, we plot the average energy usage per node per Energest time unit. What we look at here is the trend when increasing the area of the networks and how it shifts for the two different number of nodes. We observe in \cref{subfig:energy-50-nodes} that the energy usage for clustering increases with network area. The energy usage increases because the reliability drops, as we can see in \cref{subfig:reliability-50-nodes}; the decreasing reliability is due to nodes triggering resynchronisation meaning they enter the association phase which uses more energy since nodes do not sleep when they are trying to associate with the network.

When comparing 50 to 200 nodes we notice a higher energy usage on average for 200 nodes, this is expected for \atwo{} since more nodes imply more communication and longer rounds, which directly leads to higher energy consumption. Clustering shows

\begin{figure}[bt]
    \centering
        \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth, keepaspectratio]{figure/Results/ChaosComparison/Flocklab/FlocklabComparison_Latency.pdf}
        \caption{Latency.}
        \label{subfig:flocklab-latency}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth, keepaspectratio]{figure/Results/ChaosComparison/Flocklab/FlocklabComparison_Reliability.pdf}
        \caption{Reliability.}
        \label{subfig:flocklab-reliability}
    \end{subfigure}
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth, keepaspectratio]{figure/Results/ChaosComparison/Flocklab/FlocklabComparison_Energy.pdf}
        \caption{Energy.}
        \label{subfig:flocklab-energy}
    \end{subfigure}
    \caption{The mean and standard deviation for the latency, reliability, and energy consumption for the Flocklab tests.}
    \label{fig:flocklab-results}
\end{figure}

a small advantage in the 200 network \cref{subfig:energy-200-nodes}, compared to \atwo{}; each cluster can reach consensus in fewer slots than the whole network can, and it is probably the reason why our average energy usage is slightly lower in all topologies. Besides these observations the trends in 50 and 200 nodes are similar.



\subsection{Flocklab}
We evaluate the \atwo{} system with and without clustering on the Flocklab testbed. We run both protocols four times for 30 minutes and and present the results from these tests in \cref{fig:flocklab-results}. It is clear that \atwo{} outperforms our solution in all metrics, achieving a significantly lower latency, higher reliability, and lower energy consumption.

There are probably several reasons for the difference in performance. First, our clustering solution is sometimes unstable, especially in smaller, sparser networks. In a small network and the presence of external interference, our solution does not perform well. Second, the Flocklab network is probably not a good candidate for clustering. As we have discussed previously, applying clustering to a network without taking into account its density and node count will produce poor results.

Additionally, we can look at the topology of Flocklab, which we show in \cref{fig:flocklab-topology}, to further explore our results. During the tests we run, the nodes located in the top left corner (1, 2, 4, 8, and 15) spend almost all of their time trying to associate with the network, unless the Clustering service elects one of them as CH. When one of them is a CH, we get marginally better results since they have an easier time to get the correct max value. This shows that, in some cases, our clustering can be heavily topology dependent.

One thing to note is that the mean reliability for \atwo{} is approximately $86\%$. However, in the original evaluation of Chaos, Al Nahas et al.~achieved reliability of $99.9\%$ \cite{chaos-introduction-paper}. The large difference in reliability is mainly because we measure reliability differently. We have not collected the data needed to confirm their reliability results with the original reliability definition from Landsiedel et al.~\cite{chaos-introduction-paper}. However, the results presented from Cooja simulations in \cref{fig:reliability-result} does hold up to the results presented by Al Nahas et al.~\cite{a2-introduction-paper}, even under our stricter reliability definition.

\begin{figure}[bt]
    \centering
    \includegraphics[width=0.75\textwidth]{figure/Results/ChaosComparison/Flocklab/FlocklabTopology.png}
    \caption{The topology of the Flocklab testbed \cite{Lim2013-flocklab-introduction}.}
    \label{fig:flocklab-topology}
\end{figure}



\section{Discussion}
From our evaluation comparing \atwo{} and \atwo{} with clustering we conclude that using our clustering protocol performs at best as good as \atwo{} in every metric. The most significant difference is in reliability, where we consistently have significantly lower reliability than \atwo{}. For the metrics latency and energy we are on par or even in the case of latency, a bit better, than \atwo{}. However, we have to take into consideration that our clustering protocol performs only half as many application runs as \atwo{}, due to the interleaving cluster and CH rounds.

There are several possible reasons for these shortcomings, as we have mentioned previously, our clustering implementation is somewhat unstable. Because we switch which nodes considers themselves initiators, the network can end up in a state where no node consider themselves initiator. In a worst-case scenario, the tests will have $0\%$ reliability, because the network has never executed the application leaving us with no reliability data. This scenario may occur at any point during the execution, but we observe it happen more often at the times of reclustering when the dedicated initiator should return as coordinator, which is at round 200 and 400.

However, there also exist test runs where our reliability is comparable to \atwo{}; notably we got up to $98.6\%$ reliability for some of the tests. This demonstrates that we can achieve good performance with our clustering protocol under the right circumstances. However, it is difficult to tell what exactly those circumstances are. What we can observe is that the good runs generally create two or three clusters, all of which are relatively similar in their sizes. However, some runs with the same properties provide results with low reliability. Thus, a more thorough examination of the communication within clusters is required since we cannot draw conclusions only from the number of CHs elected and the number of nodes in each cluster.

We illustrate the fact that networks with very similar clustering can have drastically different reliability in \cref{fig:comparing-good-and-bad-clustering}. The network to the left in the figure (\cref{subfig:bad-clustering}) achieved reliability of $51.2\%$ while the network to the right in the figure (\cref{subfig:good-clustering}) achieved reliability of $98.6\%$. If we look at the heat map over which application is running in the networks at different times in \cref{fig:application-map-comparing-good-and-bad-clustering} we observe that in one of the networks the join application is scheduled periodically throughout the entire run, but not in the other network. Note that we compare rounds 1-199 in one network and rounds 200-399 in the other network, but this does not affect the comparison.

To understand why the bad test intermittently schedules the join application we have to look at the differences in the clusters in \cref{fig:comparing-good-and-bad-clustering}. Both networks are overall similarly clustered, the clusters are approximately the same size and located in the same places. However, two nodes have changed clusters, node 23 and node 40. When we looked at the raw data for this particular test run, we saw that it was primarily node 23 that had trouble joining its cluster, probably because it was too far away to get enough packets from its cluster. Therefore, it never could complete the join protocol. In contrast, in the good network, this does not happen, even though the two tests have created similar clusters.

\subsection{Calculating the Reliability of a Test}
The way we chose to calculate the reliability of a test may have been unnecessarily strict. What follows are three scenarios where our reliability calculations might have given unnecessarily bad results.

First, in \cref{subsec:evaluation-reliability} we look at \cref{fig:reliability-result} and observe that the reliability of the clustered network topology in an area of 100x100 with 50 nodes is a lot higher than for 200 nodes. The reason for this significant difference in reliability is due to that the network with 200 nodes schedules the Join service intermittently during the execution of the test, as seen when comparing the executions of the 50 nodes (\cref{fig:application-map-50-nodes}) to the 200 nodes (\cref{fig:application-map-200-nodes}). The pink dots represent the Join service, and the yellow dots represent the max application. However, scheduling the Join service should be an expected operation for a network to provide basic fault tolerance. 
\newpage

\begin{figure}[bt]
    \centering
        \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth, keepaspectratio]{figure/Results/Discussion/ClusteringExample50nodes1000x1000maxOffRun1Bad.pdf}
        \caption{Clustering 50 nodes, achieving a reliability of $51.2\%$.}
        \label{subfig:bad-clustering}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth, keepaspectratio]{figure/Results/Discussion/ClusteringExample50nodes1000x1000maxOffRun5Good.pdf}
        \caption{Clustering 50 nodes, achieving a reliability of $98.6\%$.}
        \label{subfig:good-clustering}
    \end{subfigure}
    \caption{An example of two networks with an area of 1000x1000 and very similar clustering but with a significant difference in reliability.}
    \label{fig:comparing-good-and-bad-clustering}
\end{figure}



Second, because we schedule CH rounds statically every other round, the network could run the first instance of the max application during a CH round; it depends on how many join rounds the network required after the demote phase. If the max application run first in a CH round, and the node with the highest ID is not a CH, then this round will automatically fail. We do not take this into account when calculating the reliability of a test run since it should happen infrequently.

Third, the way we calculate reliability can be argued to have an unavoidably decreasing trend when the amount of nodes in the network increases. If a single node fails, the whole round counts as a fail for all clusters, even if some are successful. Hence, more nodes lead to a higher probability of a single failure, which means a higher probability for a failed round.
\newpage
\input{figure/Results/Discussion/applicationmap-50nodes-1000x1000-max-off-run1vs5-bad-vs-good.tex}

\input{include/backmatter/Appendix_50_nodes_application_heatmap.tex}
\input{include/backmatter/Appendix_200_nodes_application_heatmap.tex}

