\chapter{Design}
\label{chap:design}
In this section, we explain the design behind our clustering implementation and put its objectives and properties into context. First, we provide an overview of the clustering algorithm. Second, we go into detail about the clustering service explaining the CH election algorithm. Third, we describe a demotion service which succeeds the clustering service, tidying up suboptimal clusters. Fourth, an explanation of how nodes pick cluster heads from the elected set of CHs. Last, we explain how communication works between CHs and how clusters avoid interfering with each other's communication.

\section{Clustering Overview}
Our clustering process consists of several distinct phases. First, the clustering algorithm runs, and every node simultaneously tries to do three things. Learn about the network topology, decide if they should become CH, or select an appropriate cluster to join. Second, each elected CH runs the join service from \atwo{} inside their clusters. Third, all clusters which are deemed to be too small are removed from the network, and all nodes in the removed clusters select another cluster. Fourth, each CH rerun the join service to determine the final set of nodes in their cluster. This process is repeated at some interval of rounds to accommodate for changing energy levels.

When the clustering process is complete, the network executes an application, for example, the Max aggregate application mentioned previously. At this point we split the rounds in \atwo{} into \emph{cluster} and \emph{cluster head} rounds. In cluster rounds, all clusters will use one round to execute an application to completion separately. In cluster head rounds, each CH will propose the result agreed upon by their cluster in the previous round and agree on a global value. This repeats indefinitely alternating between \emph{cluster} and \emph{cluster head} rounds until the clustering process starts again.

\section{The Clustering Service}
In this section, we begin by describing the cluster head election algorithm and how the service forms the clusters. Next, we explain the final phase of the clustering service, executed in the last three rounds of the clustering service. Finally, we detail the different parameters available to configure the behaviour of the clustering process.


\subsection{Election of Cluster Heads}
The core part of our clustering service is the election of cluster heads. The election process is probabilistic and attribute based, derived from HEED \cite{Younis2004-HEED}. Each node uses its residual energy to weigh its starting probability to elect itself as CH. In every round, all nodes have a probability to announce themselves as CH, and they double that probability every round until it reaches $1$; during this time, all nodes have either heard another CH they can join or elected themselves as CH.

The clustering service is executed over several rounds during which it converges on a stable set of cluster heads in constant time, ensured by HEED \cite{Younis2004-HEED}. The number of consecutive clustering service rounds to form stable clusters requires configuration before the network is started since it depends on the topology and the lower bound of the nodes starting probability to become cluster head.

To calculate the initial probability ($CH_{prob}$) for every node to announce themselves as CH we use the formula in \cref{eq:ch-election-probability} taken from HEED \cite{Younis2004-HEED},

\begin{equation}
    CH_{prob} = C_{prob} * \frac{E_{residual}}{E_{max}}.
    \label{eq:ch-election-probability}
\end{equation}

Where $C_{prob}$ is an initial probability which is used to get the process started, it does not have any impact on the final number of cluster heads \cite{Younis2004-HEED}. $E_{residual}$ is the residual energy of the node and $E_{max}$ is the maximum possible energy of the node. Furthermore, this probability is bounded by a constant lower bound, $p_{min}$, which ensures that $CH_{prob}$ reaches 1 in constant time.

The algorithm that decides if a node should elect itself as a CH is split into two phases. The first phase executes \cref{alg:ch-decision} at the end of each cluster service round and it stretches throughout all except the last three cluster service rounds. During the last three rounds, the final phase, shown in \cref{alg:ch-final}, is executed instead; we explain the second phase in detail in \cref{subsec:final-phase}. The first phase probabilistically elects a set of CHs purely based on their residual energy, nodes with higher residual energy has a higher chance of becoming CH. In this phase, every node also chooses a cluster to join.

There are two phases for a CH in our clustering algorithm. Tentative and final. A CH is tentative until its probability of becoming CH reaches 1, when it becomes final. This distinction is made to prune out nodes which have a lower residual energy. Since the clustering service runs for a fixed amount of rounds, if a CH is still tentative when the clustering service reaches the final phase, it will be demoted to a normal node. 

Furthermore, to avoid having too many CHs, we control the number of CHs in the following way. A node is only allowed to announce itself as CH with probability $CH_{prob}$ if they have not seen another CH within their competition radius or they have seen another CH but have a lot of neighbours, as can be seen in \cref{alg:ch-decision} at \cref{valid-clusters-not-empty}. Competition radius is a distance metric measured in hops that a node uses to filter out CHs that are too far away. However, two nodes can both independently and at the same time make the decision to become CH, even though they are within each others competition radius. To mitigate this effect, the cluster heads announce themselves in a random slot in the first half of the next round. This gives the CHs a chance to hear another nearby CH before they announce themselves, and ensures that they do not announce themselves too late in the round to not be noticed by other CHs. If they hear another valid CH, they join that cluster instead of announcing themselves.

\begin{algorithm}[bt]
\caption{Shows how a node elects to announce itself as cluster head.}
\label{alg:ch-decision}
\begin{algorithmic}[1]
\Procedure{heed\_repeat}{}
\If{$prevCH_{prob} \leq 1.0$}
    \If {$validCHList \neq \emptyset \text{ or } neighbourRatio \leq nodesPerClusterRatio$} \label{valid-clusters-not-empty}
        \State $myCH \gets pickBestCH(validCHList)$
        \If {$myCH = myNodeID$}
            \If{$CH_{prob} = 1.0$}
                \State $CHState \gets \text{FINAL}$
            \Else
                \State $CHState \gets \text{TENTATIVE}$
            \EndIf
        \EndIf
    \ElsIf {$CH_{prob} = 1.0$}
        \State $CHState \gets \text{FINAL}$
    \ElsIf {$random(0, 1.0) \geq CH_{prob}$}
        \State $announcementSlot \gets random(1, maxAnnouncementSlot)$
    \EndIf
    
    \State $prevCH_{prob} \gets CH_{prob}$
    \State $CH_{prob} \gets 2CH_{prob}$
\EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{The Final Phase}
\label{subsec:final-phase}
The final phase in the clustering service executes during the last three rounds of the clustering service. It performs two important functions, demoting CHs that are only \emph{tentative}, and ensuring that CHs cover the whole network. We show the pseudo code for the final phase in \cref{alg:ch-final}.

The final phase serves the same purpose in both our algorithm and HEED but CHs are tentative for different reasons. In HEED the only reason a CH is tentative in the final phase is if it announced itself as CH and then found another CH that it determined was a lower cost to join than being CH itself, according to some cost function. In HEED, the cost function is based on the communication cost between nodes and the CHs \cite{Younis2004-HEED}. In our algorithm, a CH is instead tentative in the final phase if it does not have enough energy to reach a probability of $1.0$ in time before the final phase starts. 

Additionally, the final phase ensures that the whole network is covered. As can be seen in \cref{alg:ch-final}, if a node cannot pick any CH that it considers valid it will announce itself as CH. This guarantees that cluster heads cover the whole network. However, there is a drawback to this design which can happen if either the parameters to the clustering service are poorly configured, or we are applying clustering to a network not suited for clustering, such as very sparse networks. For example, if the clustering service is scheduled for too few rounds, so that no CH can reach $CH_{prob} = 1.0$ then every node in the network will consider itself uncovered in the final phase and announce themselves as CHs. Having a network with only CHs is equivalent to having a network with no clustering at all and will only waste energy and time since the network will still interleave cluster and cluster head rounds.

\begin{algorithm}[bt]
\caption{The final phase of the clustering algorithm.}
\label{alg:ch-final}
\begin{algorithmic}[1]
\Procedure{heed\_final\_phase}{}
    \If{$CHState \neq \text{FINAL}$}
        \If {$validCHList \neq \emptyset$}
            \State $CHState \gets \text{NOT\_CLUSTER\_HEAD}$
            \State $myCH \gets pickBestCH(validCHList)$
        \Else
            \State $CHState \gets \text{FINAL}$
        \EndIf
    \EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Configuration Parameters}
\label{design:configuration-parameters}
In this section we explain the different parameters the clustering service takes into account when deciding which nodes are cluster heads, the parameters are listed in \cref{table:configuration-parameters}. We evaluate the effect of changing these parameters in \cref{chap:evaluation}.

\paragraph*{Competition radius,} measured in hop count, is a measurement of how far away a cluster head can be for a node to join that cluster head. In a small and dense network, a small competition radius will make the network less dense by partitioning it into a higher number of clusters and thus lower the number of packet collisions. On the other hand, in a large and sparse network, a high competition radius will keep the number of cluster heads down to ensure that the clusters do not get too small. 


\paragraph*{Minimum cluster size} is the number of nodes, including the cluster head, that has to be a part of the cluster for it to be considered valid. If a cluster has too few nodes, one of the three following scenarios is likely to have occurred. First, the network is small or very sparse, if the network only contains a few nodes clustering could be detrimental to the performance and reliability of the network and should be turned off. Second, the cluster head can be close to the edge of the network or far away from other nodes. Third, the cluster head can be close to another cluster head that most of the nodes around them elected to join instead. In both the second and the third case our algorithm elected a bad cluster head which can be pruned relatively easily when the clustering process is complete. We explain how this pruning is done in \cref{sec:demoting-cluster-heads}.


\paragraph*{Nodes per cluster ratio} is not a strict limit on the number of nodes in a cluster. Instead, it is a limit on the number of cluster heads that can announce themselves depending on the number of neighbours they have. The purpose of this parameter is to make dense networks sparser. Without this parameter, a node that has heard of another cluster head within its competition radius will never announce itself as a cluster head. With this parameter a node with more neighbours than the \emph{nodes per cluster ratio} will still be able to announce itself, dividing the neighbours between itself and the other CH, which makes the network less dense.

\begin{table}[bt]
\centering
\caption{List of configurable parameters for the clustering service.}
\label{table:configuration-parameters}
\begin{tabular}{l}
\textbf{Parameter name}      \\ \hline
Competition Radius      \\
Minimum Node Count      \\
Nodes Per Cluster Ratio
\end{tabular}
\end{table}

\section{Demotion of Cluster Heads}
\label{sec:demoting-cluster-heads}
When a cluster head is demoted it becomes a normal node and all other nodes that have joined its cluster, including itself, finds another cluster to join. There are two ways a CH can be demoted.  First, CHs will automatically be demoted at the beginning of the clustering service. Second, if a node announces itself as CH but fewer nodes than \emph{minimum cluster size} join its cluster it will also demote itself.

The clustering service is designed to be run periodically to adapt to changes in the network, such as node deaths and nodes depleting their energy faster than others. To facilitate this, if there are any previous CHs when the clustering service starts they are demoted. We also reset all statistics collected before this point to give nodes that might have depleted their energy faster than others a chance to not be chosen as CH again.  

Additionally, the demote service is designed to be run after the consistent group membership protocol provided by \atwo{} \cite{a2-introduction-paper} has converged on a result for each cluster. The purpose of the demote service is to enforce a \emph{minimum cluster size}. At this point, each CH knows how many nodes joined their cluster and if the number of nodes falls below the threshold \emph{minimum cluster size} they will announce themselves as demoted and any node that joined its cluster will have to find a new cluster.

The problem the demote service tries to mitigate is when cluster heads are either very far from other nodes or close to other cluster heads. Due to the probabilistic nature of HEED, there is no guarantee that these CHs do not announce themselves if they have high residual energy. We evaluate the effect of this service and different values of \emph{minimum cluster size} in \cref{chap:evaluation}.


\section{Joining Clusters}
The primary objective of a node is to choose a cluster head as close as possible, to do this we define a function that tries to calculate the cluster that requires the least cost to join. Since a typical WSN is an ad hoc network, we have to gather information about the network while our clustering service is running. We keep track of two different metrics to estimate how close other nodes and cluster heads are to each node. All nodes keep track of the number of packets they receive from their neighbours; a neighbour is a node which can be reached in a single hop. In addition, the nodes keep track of the number of hops required to reach each cluster head. The nodes will only consider cluster heads that have a hop count smaller than the competition radius and then chose the cluster head that they received the most packets from if there exist several valid cluster heads.

We argue that the number of received packets from a node is a reasonable approximation of the distance to that node, due to the capture effect \cite{Lee2007-capture-effect}. The signal strength decreases as the distance increases between two nodes and the capture effect state that if there is a sufficient difference in signal strength between two packets the higher signal packets will always be received correctly and the other ignored. This effect results in more packets received from nodes closer than far away.

However, the nodes only count the number of received packets from direct transmits. A two-hop (or further) packet receive count is not implemented. This means that when networks run with higher competition radii, if no CH is in one-hop range for a node, then that node will choose a CH randomly from all CHs which are closest to it since the nod has no statistics on the number of packets from the CHs.

In \cref{alg:ch-filter-cluster-heads} we show the pseudo code that determines which cluster head a node picks when joining a cluster. The algorithm takes into account two metrics, the distance (hop count) to the CH and the number of packets received from it. A node can only choose a CH within the defined competition radius. Additionally, a node picks the cluster head from which it has received the most packets.

\begin{algorithm}
\caption{Pick the best cluster head according to our cost function.}
\label{alg:ch-filter-cluster-heads}
\begin{algorithmic}[1]
\Procedure{pickBestCH}{}
    \ForAll {$CH \in clusterHeadList$}
        \If {$CH.hopCount \leq competitionRadius$}
            \If {$CH.received\_packets = max(received\_packets)$}
                \State $chosenCH \gets CH$
            \EndIf
        \EndIf
    \EndFor
    \State {\Return {$chosenCH$}}
\EndProcedure
\end{algorithmic}
\end{algorithm}


\section{Communication}
There are two separate instances of communication happening in a clustered network. The communication inside a cluster, intra-cluster communication; and the communication between cluster heads, inter-cluster communication. The communication primitive is similar to the original \atwo{} design with some modifications.


\subsection{Intra-cluster}
The intra-cluster communication is the communication that happens within a cluster between one CH and all the nodes that have joined its cluster. During this phase of the protocol, the CH acts as the initiator in the cluster, and it is responsible for initiating the communication and collecting the final result of the application running within the cluster. We use the channel hopping functionality provided by the \atwo{} Synchrotron \cite{a2-introduction-paper} to make each cluster jump between channels in a sequence to minimise foreign interference that can be caused by a single frequency band being overused. However, we also split each cluster into different channels to minimise interference between clusters. Each cluster applies a unique offset to their channel hopping sequence so that the clusters all use different channels.

The number of clusters that we can split in this way is limited by the number of available radio channels, which currently is 16. If there exist more clusters than channels, the clusters will overlap. We do not currently take any care to split the channels between clusters in any way but having more clusters will not cause the communication to break down since we filter out packets from other clusters. We explain in more detail how this is done in \cref{subsec:separating-the-clusters}.


\subsection{Inter-cluster}
The inter-cluster communication is the communication between cluster heads that takes place during CH rounds. During a CH round, the CHs propose the maximum value that its cluster has agreed on while all other nodes act as forwarders.

CH rounds are interleaved with cluster rounds which means that each cluster will first agree on a maximum local value and then the CHs will agree on a maximum global value in the next round. We decided to use separate rounds for this communication since that was the most straightforward way of getting the CHs to communicate, it is a limitation to our design. By introducing the CH rounds, we add overhead to the communication since the network has to perform two rounds instead of one to agree on a maximum value.

Another component to the inter-cluster communication is the forwarders. During a CH round, all normal nodes act as forwarders; they do not propose any values of their own, and we do not count their participation when calculating the progress of the network. Forwarders are required since our algorithm can only guarantee an upper bound of 2 on the hop count between cluster heads with the smallest competition radius of 1.


\section{Clustering Objectives and Properties}
Since we can categorise all clustering algorithms we have come across by the properties discussed in \cref{subsec:background:clustering-algorithm-categorisation} we begin by listing the clustering objectives and properties we wish to fulfil and motivate why we have made those choices.


We have several primary and secondary objectives for our clustering implementation, and they are listed in \cref{fig:primary-and-secondary-objectives}. Scalability is the first objective, both Chaos and the \atwo{} protocol have a restriction on the maximum number of nodes that can participate in the network \cite{chaos-introduction-paper}. Clustering can improve scalability by partitioning the network into smaller subsets. Our second primary objective is maximal network lifetime, which is a common challenge in WSNs \cite{Afsar2014-clustering-survey, NikolaosA.Pantaziz2007-wsn-power-survey} and clustering is known to increase the efficiency of a network which directly affects its lifetime. Finally, load balancing is our last primary objective. Load balancing works by moving the CH role between different nodes, which increases the time until the first node death. Prolonging the time until first node death occurs directly increases the lifetime of the network.

\begin{table}[bt]
    \centering
    \caption{Our primary and secondary clustering objectives.}
    \begin{tabular}{l}
        \textbf{Clustering Objectives}    \\ \hline
        Scalability              \\
        Maximal network lifetime \\
        Load balancing           \\
        Collision avoidance      \\
        Increased connectivity  
    \end{tabular}
    \label{fig:primary-and-secondary-objectives}
\end{table}

\begin{table}[bt]
\centering
\caption{Our properties related to clusters, cluster heads and the clustering process.}
\label{table:clustering-design-properties}
\begin{tabular}{ll}
\multicolumn{1}{c}{\textbf{Cluster properties}}             & \multicolumn{1}{c}{\textbf{Cluster head properties}} \\ \hline
\multicolumn{1}{l|}{Unequal cluster size}                   & Stationary nodes                                     \\
\multicolumn{1}{l|}{Variable cluster count}                 & Homogeneous nodes                                    \\
\multicolumn{1}{l|}{Single/multi-hop intra-cluster communication} & CH takes on normal duties                            \\
\multicolumn{1}{l|}{Multi-hop inter-cluster communication}  & \textbf{}                                            \\
                                                            &                                                      \\
\multicolumn{2}{c}{\textbf{Clustering process properties}}                                                         \\ \hline
Distributed clustering process                              & Proactive nature                                     \\
Attribute based CH election                                 & Dynamic clustering                                   \\
Constant algorithm complexity                               &                                                     
\end{tabular}
\end{table}

Additionally, we have two secondary objectives. Collision avoidance occurs because the clusters will communicate on different radio channels, which reduces potential interference between the clusters. Furthermore, we gain increased connectivity since the requirement for the network to be considered connected is weakened. That is, each node only needs to have a connection with its cluster and a CH require a connection with its cluster and the other CHs.


Furthermore, our clustering implementation fulfils several properties, listed in \cref{table:clustering-design-properties}. First,  we will have unequal cluster sizes. More specifically, we will enforce a minimum cluster size by only letting a node announce itself as CH if it has a minimum number of followers. Furthermore, we enforce a nodes per cluster ratio; however, this parameter is dependent on network density. The second parameter is variable cluster count, the number of clusters will depend on many variables such as network topology and competition radius. Third, the intra-cluster communication will depend on the competition radius. If competition radius is one, the nodes will have a single-hop communication with the cluster head. However, it is possible for two nodes in the same cluster to have multi-hop communication. For example, the two nodes might require the CH to route communication between them. Finally, inter-cluster communication will be multi-hop. CHs will communicate with the normal \atwo{} protocol but will require non-CH nodes to forward their messages during CH-rounds.

Similarly, we have cluster head properties. First, our design will only focus on stationary nodes. Second, all nodes are homogeneous. However, if we deploy a node with more resources (such as an improved battery) with our protocol, it will have an advantage in our election process since the election favours nodes with higher battery levels. Finally, the role of the CH in our design will be to perform the same work as ordinary nodes in addition to the work required by a CH. Additionally, normal nodes perform a forwarding service for the CHs during CH-rounds.

Finally, we have properties for the clustering process. First,  each node executes the clustering process in a distributed fashion; this will lead to a more scalable network. Second, our election process will be attributed based and take into account attributes such as residual energy to elect CHs to work towards maximising the network lifetime. Third, we will have a constant algorithm complexity; this is because we use local decisions to elect CHs. Fourth, the clustering process will be proactive to accommodate \atwo{}'s ability to schedule multiple applications in the network. Lastly, since our clustering process is attribute based it will be dynamic, meaning it takes into account the current condition of the network.


