\chapter{Related Work}
\label{chap:related-research}
There is a variety of research on how to increase scalability and energy efficiency in Wireless Sensor Networks. In this section, we present a few clustering algorithms to highlight their key ideas, similarities, and differences. We also present research that improves the scalability of the Chaos protocol using estimation vectors. Finally, we end with a discussion and comparison to our clustering process.

\section{UHEED}
In \cref{subsection:background-heed}, we present HEED, a probabilistic, equal clustering algorithm. Unequal HEED (UHEED) \cite{Ever2012-UHEED}, presented in 2012, builds on HEED and makes the algorithm unequal to solve the hot spot problem. This is the only difference to the HEED algorithm.

UHEED creates smaller clusters closer to the base station to mitigate the hot spot problem. The idea is that CHs that are closer to the base station will need to route more traffic from other clusters and thus have a higher energy consumption than a CH further away from the base station.

The clusters get progressively smaller by defining a \textit{competition radius}, which is the area a node considers when choosing CH. UHEED uses a formula to calculate the competition radius, developed by Lie et al.~\cite{Li2005-EEUC}, that uses the maximum distance between nodes and the distance to the base station as parameters. Evaluations of UHEED showed that it has an equal or better performance in virtually every case when compared to state of the art clustering algorithms \cite{Ever2012-UHEED}.

\section{BCDCP}
\begin{newtext}
Base-station Controlled Dynamic Clustering Protocol \cite{Muruganathan2005-bdccp-centralized-clustering} is a centralised and dynamic clustering protocol presented in 2005. The clusters are created by a base station, with much higher energy levels and better computational power than the nodes, which enables the algorithm to leverage global properties of the network to create efficient clusters. 

BCDCP uses a four-step process, executed iteratively until the target number of clusters is reached. The process begins by collecting a set $\mathbb{S}$ of nodes with higher than average energy levels, which are candidate CHs. Then execute the following steps.

\begin{enumerate}
    \item Choose two nodes $s_1$ and $s_2$ from $\mathbb{S}$ that have a maximum separation distance.
    \item Group all remaining nodes in the current cluster (this includes all nodes in the first iteration) with $s_1$ or $s_2$, depending on which is closest.
    \item Balance the two groups so that they are approximately equal; this forms the two subclusters.
    \item Split $\mathbb{S}$ into subsets $\mathbb{S}_1$ and $\mathbb{S}_2$ according to the subcluster groupings created in step 3.
\end{enumerate}

Performing these steps produces a set of clusters which are distributed uniformly throughout the network, where the communication cost within a cluster is minimised \cite{Muruganathan2005-bdccp-centralized-clustering}.

BCDCP is evaluated by simulations conducted in Matlab, and compared to several other clustering algorithms. From these simulations, Muruganathan et al.~conclude that BCDCP achieves better performance regarding energy usage, and first node death compared to other state of the art clustering algorithms. However, they note that BCDCP is most effective for networks spread over a considerable distance, and the benefit of using BCDCP is less for smaller networks.

\end{newtext}

\section{LEACH}
A reactive clustering protocol developed in 2002 is \textit{Low-Energy Adaptive Clustering Hierarchy} (LEACH) \cite{Heinzelman2002-leach}. Heinzelman et al.~designed LEACH for applications that measure something in the environment where the interesting results are not the individual values but some computation using those values. The clustering algorithm in LEACH is distributed and probabilistic with the primary objective of saving energy.

The LEACH algorithm is divided into rounds. Rounds in LEACH are conceptually different to rounds in Chaos. Each round begins with a setup phase that forms the clusters, followed by a steady state phase where nodes collect data and CHs aggregate and forward that data to the base station. In the setup phase, each node $i$ has a probability $P_i(t)$ at time $t$ to announce themselves as CH. This probability depends on a number $k$, which is the target number of clusters. In a network that contains $N$ nodes, the probability $P_i(t)$ is chosen so that the following formula holds:

$$E[\#CH] = \sum_{i=1}^N P_i(t) * 1 = k$$.

Also, to maximise network lifetime, each node takes on equal responsibility of becoming a CH. Thus, if there are $N$ nodes in the network, each node will, on average, be CH every $N/k$ rounds.

The nodes which have elected themselves as CH need to inform the rest of the network of this decision. They do this by sending out an advertisement message containing their ID. All other nodes will listen for these messages and map each CH with the energy required to transmit a packet to it. The node then picks the CH that requires the lowest transmission energy.

Leach is evaluated and compared to other clustering algorithms in simulations. The results show that LEACH achieves better energy usage and data throughput when compared to other distributed clustering algorithms as well as static clustering of the network. However, it is outperformed by LEACH-C, a centralised version of LEACH.

\section{Extreme Chaos}
Chronopoulos \cite{Chronopoulos2016-extreme-chaos} address the scaling bound on the number of participating nodes described in \cref{sec:chaos-a2-background}, and introduce flow control to limit the high number of initial transmitters in a Chaos round. The flags field requires at least one bit per node, but together with the additional information required in the packet (synchronisation, error-detection and payload), the number of possible participating nodes is less than a thousand \cite{chaos-introduction-paper}. Chronopoulos also discuss how the flags field affects the latency of transmissions. They note that increasing the network size by a factor of 10 increases the delay in the network by a factor of 100 \cite{Chronopoulos2016-extreme-chaos}. Last, they note that the flags field also imposes an overhead on the energy consumption when it is transmitted.

\subsection{Estimation Vector}
To replace the flags field, Chronopoulos propose to use an estimation vector, which is the most prominent part of the new protocol. Just as the flags field, the estimation vector is used for keeping track of how much of the network has contributed to the aggregate. Chronopoulos argue that extreme values spread quickly throughout the network and therefore nodes do not need to make sure they have communicated with every other node. As such nodes only need to set a percentage of the flags. However, this does not reduce the size of the flags field. The new vector estimates the number of nodes which has contributed to the aggregate, and by also estimating the network size the nodes infer the contribution ratio.
%The theory used is called a cardinality estimator[source to original paper used in extreme chaos].


%% The procedure of generating and maintaining the estimation vector is a complicated one. Each node begins by generating its starting vector of $n$ values picked from a range $[1-m]$

\subsection{Flow Control}
An additional issue addressed by Chronopoulos is the high number of transmitters at the beginning of a round, which is caused by the transmission policy of Chaos. Nodes only transmit when they receive new information, however, at the beginning of a round, there is a high probability that most nodes will receive new information. Too many transmitters cause interference and slow packet propagation since nodes need to retransmit packets. The mechanism to counter this flooding is called flow control, and its purpose is to prevent congestion from occurring by applying an \textit{exponentially decaying back-off probability} \cite{Chronopoulos2016-extreme-chaos}. The back-off is a node deciding, according to some probability, to not send during a slot in which it intended to send. The exponential decay is referring to the probability decreasing at an exponential rate throughout the round. Decreasing the congestion in early slots will speed up the progress of an aggregate process which enables a round to finish earlier.

To not have adverse effects the parameters used for calculating the next probability for back-off requires careful testing. Having a too low probability of back-off would mean the initial flooding is not constrained enough. In contrast, a too high probability means dropping later transmissions which would hinder the aggregation process. Additionally, flow control can affect large and small networks differently. In a large and sparse network, an extended period with initial high flooding can occur. Chronopoulos argue that this is because nodes far from the initiator join the network in later slots. However, Chronopoulos does not consider large multi-hop networks since their primary focus is to optimise Chaos for local neighbourhoods.

\section{Discussion}
\begin{newtext}
In this chapter, we presented three different clustering algorithms and Extreme Chaos. We discuss and compare the clustering algorithms to the HEED algorithm, and contrast the work on estimation vectors and flow control to clustering. 


The clustering algorithms presented here all exhibit different characteristics, and restrict the network in different ways to accomplish their goals or solve specific problems. UHEED, which is further work on the HEED algorithm, primarily tries to solve the hot spot problem. However, since the \atwo{} system does not currently have any concept of base stations for any of its applications, the hot spot problem does not apply in this thesis.

LEACH, on the other hand, is similar to HEED since it is also probabilistic and wants to maximise network lifetime. However, it assumes that the data the application running on the network transmits can be efficiently aggregated. LEACH also evenly distributes the CH role between all nodes in the network, which can be an undesirable property since, for example, nodes at the edges of the network might be unsuitable CHs.


Furthermore, BCDCP is a centralised algorithm, and as such, it can leverage both a global view of the network and use more resources, since it is assumed that the node responsible for the clustering has more computational power. No such node is assumed to exist by the \atwo{} system, which makes this and other centralised algorithms unsuitable. It could be feasible to employ this protocol on a typical node, however, in that case, there is a high risk that the network would not scale well.

Last, Extreme Chaos also addresses the scalability issues that the \atwo{} system exhibits. However, the approach is fundamentally different to clustering. Both the estimation vector and flow control are probabilistic mechanisms that affect Chaos in every round. A probabilistic clustering algorithm, on the other hand, only exhibits uncertainties during the clustering process, then, during normal operation of the Chaos protocol, the process is strictly deterministic.
\end{newtext}