\chapter{Related Research}
\label{chap:related-research}
There is a variety of research on clustering in Wireless Sensor Networks. In this section, we present a few influential clustering protocols such as HEED and LEACH to highlight their key ideas and their similarities and differences. We also present other research which improves the scalability of the Chaos protocol using estimation vectors. Finally, we end with a comparison to our clustering algorithm.

\section{Equal and Unequal Clustering}
\textit{Hybrid Energy-Efficient Distributed Clustering} (HEED) \cite{Younis2004-HEED} is a probabilistic, attribute-based clustering algorithm with the primary objective of saving energy. Unequal HEED (UHEED) \cite{Ever2012-UHEED} builds on HEED and makes the algorithm unequal to solve the hot spot problem. In this section, we first present the internal workings of HEED, followed by the changes introduced by UHEED.

\subsection{HEED}
Younis et al.~present HEED \cite{Younis2004-HEED}, a probabilistic, equal clustering algorithm with the primary objective of saving energy. To this end, HEED uses residual node energy as its main parameter to elect CHs and intra-cluster communication cost as its second parameter. An example of intra-cluster communication cost is the number of nodes that have already joined a cluster. In HEED, the clustering algorithm executes at regular intervals ranging from seconds to hours depending on the application running in the WSN and how significant the increase in energy consumption is when a node is a CH.

At the start of the clustering algorithm, each node sets an initial probability to, for example, $C_{prob} = 5\%$ and then apply its proportion of residual energy as weight according to the following formula to calculate the probability of becoming CH as: $$CH_{prob} = C_{prob} * \frac{E_{residual}}{E_{max}}$$ Where $E_{residual}$ and $E_{max}$ is the residual energy and the maximum possible energy respectively. The value of $CH_{prob}$ is limited by a carefully selected lower bound, inversely proportional to the value of $E_{max}$ to ensure that the algorithm terminates in $\mathcal{O}(1)$ time.

Furthermore, there are two stages for any node announcing itself as CH; a node first becomes a \textit{tentative} CH until it either resigns or becomes a \textit{final} CH. During every iteration of the algorithm, every node will double the value of $CH_{prob}$ until it reaches 1 or the algorithm terminates. A node announces itself as a tentative CH with probability $CH_{prob}$ if it has not heard from any other CH and moves on to become a final CH if $CH_{prob} = 1$. If a node reaches the end of the algorithm without selecting a final CH, it will announce itself as a final CH. Last, a CH will resign from being tentative CH if it finds another CH with lower cost.

\subsection{UHEED}
UHEED \cite{Ever2012-UHEED} builds upon HEED; the most significant change is that UHEED is an unequal clustering algorithm. The algorithm will create smaller clusters when closer to the base station to mitigate the hot spot problem. The idea is that CHs that are closer to the base station will need to handle more routing traffic from other clusters and thus have a higher energy consumption than a CH further away from the base station.

The clusters get progressively smaller by defining a \textit{competition radius} which is the area a node considers when choosing CH. UHEED uses the formula to calculate the competition radius developed by Lie et al.~\cite{Li2005-EEUC}. The formula uses the maximum distance between nodes and the current distance to the base station as parameters. Evaluations of HEED and UHEED showed that UHEED had an equal or better performance in virtually every case when compared to the state of the art \cite{Ever2012-UHEED}.


\section{LEACH}
A significant reactive clustering protocol developed in 2002 is LEACH \cite{Heinzelman2002-leach}. Heinzelman et al.~designed LEACH for applications that measure something in the environment where the interesting results are not the individual values but some computation using those values. The clustering algorithm in LEACH is distributed and probabilistic with the primary objective of saving energy.

The LEACH algorithm divides into rounds. Rounds in Leach is conceptually different to Chaos. Each round begins with a setup phase which forms the clusters, followed by a steady state phase where nodes collect data and CHs aggregate and forward that data to the base station. In the setup phase, each node has a probability $P_i(t)$ at time $t$ to announce themselves as CH. This probability is dependent on a number $k$, which is the target amount of clusters. In a network that contains $N$ nodes, we choose the probability $P_i(t)$ so that the following formula holds.

$$E[\#CH] = \sum_{i=1}^N P_i(t) * 1 = k$$

Also, to maximise network lifetime, each node takes on an equal responsibility of becoming a CH. Thus, if there are $N$ nodes in the network, each node will on average be CH every $N/k$ rounds.

When all nodes have decided if they are a CH, they need to inform the rest of the network of this decision. They do this by sending out an advertisement message containing their ID. All other nodes will listen for these messages and map each CH with the energy required to transmit a packet to it. The node then picks the CH with the lowest transmission energy.

\section{Extreme Chaos}
Chronopoulos \cite{Chronopoulos2016-extreme-chaos} addresses the scaling bound on the number of participating nodes described in \cref{sec:chaos-a2-background}. In particular, Chronopoulos addresses the completion vector which is bounded by the 802.15.4 packet size of 127 bytes \cite{IEEE-802-15-4}. The completion vector in the \atwo{} protocol is the flags field created by the join service as detailed in \cref{subsec-flags-field}. The vector requires only one bit per member, but together with the additional information required in the packet (synchronisation, error-detection and payload), the number of possible participating nodes is less than a thousand \cite{chaos-introduction-paper}. Chronopoulos also discusses how the flags field affects the latency of transmission. They note that increasing the network size by a factor of 10 increases the delay in the network by a factor of 100 \cite{Chronopoulos2016-extreme-chaos}. Last, they note that the completion vector also imposes an overhead on the energy consumption when it is transmitted. Since transmission requires a significant amount of energy, the design of the completion vector is worth restructuring.

\subsection{Estimation Vector}
From the arguments above Chronopoulos reasons that the most significant improvement for the Chaos protocol is to replace the flags field. Hence, they propose an estimation vector which is the most prominent part of the new protocol. Just as the flags field, the estimation vector is used for keeping track of how much of the network has contributed to the aggregate. Chronopoulos argues that extreme values spread quickly throughout the network and therefore nodes do not need to make sure they have communicated with every other node. As such we only need to set a percentage of the flags. However, this does not reduce the size of the flags field. The new vector tries to estimate the number of nodes which has contributed to the aggregate, by also estimating the network size the nodes infer the contribution ratio. 
%The theory used is called a cardinality estimator[source to original paper used in extreme chaos].


%% The procedure of generating and maintaining the estimation vector is a complicated one. Each node begins by generating its starting vector of $n$ values picked from a range $[1-m]$

\subsection{Flow Control}
An additional issue addressed by Chronopoulos is the high number of transmitters at the beginning of a round whom together flood the network. The mechanism put in place to counter this flooding is called flow control, and its purpose is to prevent congestion from occurring by applying an \textit{exponentially decaying back-off probability} \cite{Chronopoulos2016-extreme-chaos}. The back-off is a node deciding, according to some probability, to not send during a slot at which it was originally scheduled to do so. The exponential decay is referring to the probability decreasing at an exponential rate throughout the round. Stopping congestion in early slots will make an aggregate progress quick which enables a round to finish earlier.

To not have adverse effects the parameters used for calculating the next probability for back-off requires careful testing. Having a too low probability of back-off would mean the initial flooding is not constrained enough. In contrast, a too high probability means dropping later transmissions which would hinder the aggregation process. Similarly, flow control can affect large and small networks differently. In a large network, an extended period with initial high flooding can occur. Chronopoulos argues that this is because nodes far from the initiator join the network in later slots. However, Chronopoulos does not consider large networks since their primary focus is to optimise Chaos for local neighbourhoods.

\section{Discussion}
In this thesis, we base our clustering algorithm on the one presented in HEED. However, we make some changes to fit the \atwo{} protocol better but these are mostly regarding specific implementation details. We present these changes in \cref{chap:design} and \cref{chap:implementation}. We do not use any of the modifications presented about UHEED in our implementation since we do not consider base stations.

Furthermore, the LEACH protocol is similar to HEED in that it is also probabilistic; however, its scope is more narrow. It has a dependency on the applications that are running the protocol which we found limiting for our work. Also, the algorithm tries to enforce that every node in the network becomes CH eventually, this is not a desirable property for us since there might be nodes in the network which would be bad CHs.

Lastly, Extreme Chaos is another approach that addresses the scalability issues that Chaos has due to the flags field. However, there is one significant difference. By using an estimation vector rather than the complete flags field there is never a guarantee that the network has reached completion, there will always be some uncertainty. Using clustering instead could give a suboptimal clustering, but there will never be any uncertainty as to if the network has completed or not. Finally, using clustering in a WSN is an explored area with much research behind it. 
